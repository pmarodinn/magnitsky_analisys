{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d378517",
   "metadata": {},
   "source": [
    "# An√°lise de Impacto Econ√¥mico: Lei Magnitsky e o Mercado Brasileiro\n",
    "\n",
    "**An√°lise Quantitativa do Impacto Potencial de San√ß√µes da Lei Magnitsky no Ibovespa**\n",
    "\n",
    "**Autor:** Pedro Schuves Marodin  \n",
    "**Data:** 31 de julho de 2025  \n",
    "**Vers√£o:** 1.0\n",
    "\n",
    "---\n",
    "\n",
    "## Resumo Executivo\n",
    "\n",
    "Este notebook implementa uma an√°lise abrangente do impacto econ√¥mico potencial da aplica√ß√£o da Lei Global Magnitsky a uma figura pol√≠tica de alto escal√£o no Brasil, utilizando:\n",
    "\n",
    "- **Estudo de Eventos** para medir impactos anormais no mercado\n",
    "- **Machine Learning N√£o Supervisionado** para identificar padr√µes em casos hist√≥ricos\n",
    "- **Machine Learning Supervisionado** para predi√ß√£o de cen√°rios\n",
    "- **An√°lise de Sentimento** para incorporar fatores comportamentais\n",
    "\n",
    "### Metodologia\n",
    "1. Coleta de dados financeiros brasileiros e globais\n",
    "2. An√°lise de casos hist√≥ricos de san√ß√µes Magnitsky\n",
    "3. Implementa√ß√£o de estudo de eventos com modelo CAPM\n",
    "4. Clustering de casos hist√≥ricos para identifica√ß√£o de padr√µes\n",
    "5. Treinamento de modelos preditivos\n",
    "6. Simula√ß√£o de cen√°rios para o Brasil\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53892473",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Installation\n",
    "\n",
    "Primeiro, vamos instalar e importar todas as bibliotecas necess√°rias para nossa an√°lise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00446d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar bibliotecas necess√°rias\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Instala um pacote se n√£o estiver dispon√≠vel\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Lista de pacotes necess√°rios\n",
    "required_packages = [\n",
    "    'yfinance',\n",
    "    'pandas', \n",
    "    'numpy',\n",
    "    'scikit-learn',\n",
    "    'xgboost',\n",
    "    'lightgbm',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'plotly',\n",
    "    'scipy',\n",
    "    'statsmodels',\n",
    "    'requests',\n",
    "    'beautifulsoup4',\n",
    "    'nltk',\n",
    "    'textblob',\n",
    "    'vaderSentiment',\n",
    "    'pyyaml'\n",
    "]\n",
    "\n",
    "print(\"Instalando pacotes necess√°rios...\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        install_package(package)\n",
    "        print(f\"‚úì {package}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Erro ao instalar {package}: {e}\")\n",
    "\n",
    "print(\"\\nInstala√ß√£o conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28e82a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Bibliotecas para dados financeiros\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Bibliotecas para machine learning\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, silhouette_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Bibliotecas para an√°lise estat√≠stica\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "# Bibliotecas para processamento de texto e sentimento\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Configura√ß√µes\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(\"‚úì Todas as bibliotecas importadas com sucesso!\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
    "print(f\"üìà YFinance dispon√≠vel para coleta de dados financeiros\")\n",
    "print(f\"ü§ñ Scikit-learn dispon√≠vel para machine learning\")\n",
    "print(f\"üöÄ XGBoost e LightGBM dispon√≠veis para modelos avan√ßados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4ee73",
   "metadata": {},
   "source": [
    "## 2. Data Collection from Financial APIs\n",
    "\n",
    "Nesta se√ß√£o, coletaremos dados financeiros do mercado brasileiro e global usando as APIs dispon√≠veis.\n",
    "\n",
    "### Fontes de Dados:\n",
    "- **Mercado Brasileiro:** Ibovespa (^BVSP), USD/BRL, VIX Brasil\n",
    "- **Mercados Globais:** S&P 500 (^GSPC), NASDAQ (^IXIC), VIX (^VIX)\n",
    "- **Per√≠odo:** √öltimos 5 anos para an√°lise robusta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar per√≠odo de an√°lise\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=5*365)  # 5 anos de dados\n",
    "\n",
    "print(f\"üìÖ Per√≠odo de an√°lise: {start_date.strftime('%Y-%m-%d')} a {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Definir tickers para coleta\n",
    "tickers = {\n",
    "    # Mercado Brasileiro\n",
    "    'IBOVESPA': '^BVSP',\n",
    "    'USD_BRL': 'BRL=X',\n",
    "    'VIBOV11': 'VIBOV11.SA',  # VIX Brasil (se dispon√≠vel)\n",
    "    \n",
    "    # Mercados Globais\n",
    "    'SP500': '^GSPC',\n",
    "    'NASDAQ': '^IXIC', \n",
    "    'VIX': '^VIX',\n",
    "    \n",
    "    # Outros ativos relevantes\n",
    "    'IFIX': 'IFIX.SA',  # √çndice de Fundos Imobili√°rios\n",
    "    'SELIC': 'SELIC.SA'  # Taxa Selic (se dispon√≠vel)\n",
    "}\n",
    "\n",
    "def collect_financial_data(tickers_dict, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Coleta dados financeiros usando yfinance\n",
    "    \n",
    "    Args:\n",
    "        tickers_dict: Dicion√°rio com nome e ticker\n",
    "        start_date: Data inicial\n",
    "        end_date: Data final\n",
    "    \n",
    "    Returns:\n",
    "        Dicion√°rio com DataFrames dos dados coletados\n",
    "    \"\"\"\n",
    "    data_collection = {}\n",
    "    \n",
    "    for name, ticker in tickers_dict.items():\n",
    "        try:\n",
    "            print(f\"üìä Coletando dados para {name} ({ticker})...\")\n",
    "            \n",
    "            # Baixar dados\n",
    "            stock_data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "            \n",
    "            if not stock_data.empty:\n",
    "                # Calcular retornos di√°rios\n",
    "                stock_data['Returns'] = stock_data['Adj Close'].pct_change()\n",
    "                \n",
    "                # Calcular volatilidade rolante (20 dias)\n",
    "                stock_data['Volatility_20d'] = stock_data['Returns'].rolling(window=20).std() * np.sqrt(252)\n",
    "                \n",
    "                data_collection[name] = stock_data\n",
    "                print(f\"  ‚úì {len(stock_data)} observa√ß√µes coletadas\")\n",
    "            else:\n",
    "                print(f\"  ‚úó Nenhum dado encontrado para {ticker}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Erro ao coletar {name}: {str(e)}\")\n",
    "    \n",
    "    return data_collection\n",
    "\n",
    "# Coletar todos os dados\n",
    "print(\"üöÄ Iniciando coleta de dados financeiros...\\n\")\n",
    "market_data = collect_financial_data(tickers, start_date, end_date)\n",
    "\n",
    "print(f\"\\n‚úÖ Coleta conclu√≠da! Dados dispon√≠veis para: {list(market_data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ebe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar dados coletados\n",
    "def create_market_overview(market_data):\n",
    "    \"\"\"Criar overview dos dados de mercado coletados\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Ibovespa vs S&P 500 (Pre√ßos Normalizados)', \n",
    "                       'USD/BRL Exchange Rate',\n",
    "                       'Volatilidade (Ibovespa vs S&P 500)',\n",
    "                       'Retornos Di√°rios - Ibovespa'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Pre√ßos normalizados (base 100)\n",
    "    if 'IBOVESPA' in market_data and 'SP500' in market_data:\n",
    "        ibov_normalized = (market_data['IBOVESPA']['Adj Close'] / market_data['IBOVESPA']['Adj Close'].iloc[0]) * 100\n",
    "        sp500_normalized = (market_data['SP500']['Adj Close'] / market_data['SP500']['Adj Close'].iloc[0]) * 100\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=ibov_normalized.index, y=ibov_normalized.values, \n",
    "                      name='Ibovespa', line=dict(color='blue')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=sp500_normalized.index, y=sp500_normalized.values, \n",
    "                      name='S&P 500', line=dict(color='red')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. C√¢mbio USD/BRL\n",
    "    if 'USD_BRL' in market_data:\n",
    "        usd_brl = market_data['USD_BRL']['Adj Close']\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=usd_brl.index, y=usd_brl.values, \n",
    "                      name='USD/BRL', line=dict(color='green')),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Volatilidade\n",
    "    if 'IBOVESPA' in market_data and 'SP500' in market_data:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=market_data['IBOVESPA'].index, \n",
    "                      y=market_data['IBOVESPA']['Volatility_20d'],\n",
    "                      name='Vol Ibovespa', line=dict(color='blue', dash='dot')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=market_data['SP500'].index, \n",
    "                      y=market_data['SP500']['Volatility_20d'],\n",
    "                      name='Vol S&P 500', line=dict(color='red', dash='dot')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # 4. Histograma de retornos do Ibovespa\n",
    "    if 'IBOVESPA' in market_data:\n",
    "        returns = market_data['IBOVESPA']['Returns'].dropna()\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=returns, name='Retornos Ibovespa', \n",
    "                        nbinsx=50, opacity=0.7),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"Overview dos Dados de Mercado\", showlegend=True)\n",
    "    fig.show()\n",
    "\n",
    "# Criar overview\n",
    "if market_data:\n",
    "    create_market_overview(market_data)\n",
    "    \n",
    "    # Estat√≠sticas descritivas\n",
    "    print(\"\\nüìà ESTAT√çSTICAS DESCRITIVAS DOS RETORNOS:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for name, data in market_data.items():\n",
    "        if 'Returns' in data.columns:\n",
    "            returns = data['Returns'].dropna()\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  Retorno m√©dio anual: {returns.mean() * 252:.2%}\")\n",
    "            print(f\"  Volatilidade anual:  {returns.std() * np.sqrt(252):.2%}\")\n",
    "            print(f\"  Sharpe Ratio:       {(returns.mean() * 252) / (returns.std() * np.sqrt(252)):.2f}\")\n",
    "            print(f\"  Skewness:           {returns.skew():.3f}\")\n",
    "            print(f\"  Kurtosis:           {returns.kurtosis():.3f}\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado foi coletado com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4942b",
   "metadata": {},
   "source": [
    "## 3. Historical Magnitsky Cases Data Preparation\n",
    "\n",
    "Nesta se√ß√£o, criaremos um dataset estruturado com casos hist√≥ricos de san√ß√µes da Lei Magnitsky para an√°lise comparativa.\n",
    "\n",
    "### Casos Hist√≥ricos Identificados:\n",
    "1. **Ramzan Kadyrov** (R√∫ssia) - 2017\n",
    "2. **Rosario Murillo** (Nicar√°gua) - 2018  \n",
    "3. **Maikel Moreno** (Venezuela) - 2017\n",
    "4. **Dan Gertler** (R.D. Congo) - 2017\n",
    "5. **Gao Yan** (China) - 2020\n",
    "\n",
    "### Features para An√°lise:\n",
    "- Profile Score (1-4): N√≠vel de import√¢ncia pol√≠tica\n",
    "- Country Risk: √çndice de risco pol√≠tico\n",
    "- Market Cap/GDP: Import√¢ncia relativa do mercado\n",
    "- CAR Magnitude: Impacto observado no mercado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632cebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar dataset de casos hist√≥ricos de san√ß√µes Magnitsky\n",
    "historical_cases = pd.DataFrame({\n",
    "    'Individual': [\n",
    "        'Ramzan Kadyrov',\n",
    "        'Rosario Murillo', \n",
    "        'Maikel Moreno',\n",
    "        'Dan Gertler',\n",
    "        'Gao Yan',\n",
    "        'Aleksandr Bortnikov',\n",
    "        'Chen Quanguo',\n",
    "        'Arkadiusz Rejmowicz'\n",
    "    ],\n",
    "    'Country': [\n",
    "        'Russia',\n",
    "        'Nicaragua',\n",
    "        'Venezuela', \n",
    "        'DR Congo',\n",
    "        'China',\n",
    "        'Russia',\n",
    "        'China',\n",
    "        'Poland'\n",
    "    ],\n",
    "    'Sanction_Date': [\n",
    "        '2017-12-20',\n",
    "        '2018-11-27',\n",
    "        '2017-05-18',\n",
    "        '2017-12-21',\n",
    "        '2020-07-09',\n",
    "        '2021-04-15',\n",
    "        '2021-03-22',\n",
    "        '2020-10-02'\n",
    "    ],\n",
    "    'Position': [\n",
    "        'Head of Chechen Republic',\n",
    "        'Vice President',\n",
    "        'Supreme Court President',\n",
    "        'Business Magnate',\n",
    "        'Party Official Beijing',\n",
    "        'FSB Director',\n",
    "        'Party Secretary Xinjiang',\n",
    "        'Regional Prosecutor'\n",
    "    ],\n",
    "    'Profile_Score': [4, 4, 4, 2, 3, 4, 3, 2],  # 1=low level, 4=top level\n",
    "    'Country_Risk': [65, 78, 85, 72, 45, 65, 45, 25],  # Higher = more risk\n",
    "    'Market_Cap_GDP': [0.4, 0.1, 0.05, 0.15, 0.65, 0.4, 0.65, 0.3],  # Market importance\n",
    "    'CAR_5_days': [-2.1, -4.8, -8.2, -1.2, -0.3, -1.8, -0.5, -0.8],  # Observed 5-day impact (%)\n",
    "    'Volatility_Spike': [15, 45, 85, 8, 2, 12, 3, 5],  # % increase in volatility\n",
    "    'Media_Sentiment': [-0.2, -0.6, -0.8, -0.3, -0.1, -0.4, -0.2, -0.2],  # Sentiment score\n",
    "    'Market_Index': [\n",
    "        'MOEX',\n",
    "        'Government Bonds', \n",
    "        'IBC Caracas',\n",
    "        'Local Mining Stocks',\n",
    "        'Shanghai Composite',\n",
    "        'MOEX',\n",
    "        'Shanghai Composite',\n",
    "        'WIG20'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Converter datas\n",
    "historical_cases['Sanction_Date'] = pd.to_datetime(historical_cases['Sanction_Date'])\n",
    "\n",
    "# Adicionar features derivadas\n",
    "historical_cases['Impact_Magnitude'] = np.abs(historical_cases['CAR_5_days'])\n",
    "historical_cases['Risk_Adjusted_Impact'] = historical_cases['CAR_5_days'] / historical_cases['Country_Risk'] * 100\n",
    "\n",
    "print(\"üìä DATASET DE CASOS HIST√ìRICOS CRIADO:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total de casos: {len(historical_cases)}\")\n",
    "print(f\"Per√≠odo: {historical_cases['Sanction_Date'].min().strftime('%Y-%m-%d')} a {historical_cases['Sanction_Date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Pa√≠ses √∫nicos: {historical_cases['Country'].nunique()}\")\n",
    "\n",
    "# Mostrar estat√≠sticas por perfil\n",
    "print(\"\\nüìà IMPACTO M√âDIO POR PERFIL:\")\n",
    "profile_impact = historical_cases.groupby('Profile_Score').agg({\n",
    "    'CAR_5_days': ['mean', 'std', 'count'],\n",
    "    'Volatility_Spike': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "profile_labels = {1: 'Baixo Escal√£o', 2: 'Empres√°rio/Oficial', 3: 'Alto Oficial', 4: 'Topo Pol√≠tico'}\n",
    "for score in profile_impact.index:\n",
    "    print(f\"  {profile_labels[score]} (Score {score}): CAR m√©dio = {profile_impact.loc[score, ('CAR_5_days', 'mean')]:.1f}%\")\n",
    "\n",
    "# Visualizar casos hist√≥ricos\n",
    "display(historical_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a57a98",
   "metadata": {},
   "source": [
    "## 4. Event Study Methodology Implementation\n",
    "\n",
    "Implementa√ß√£o do framework de estudo de eventos conforme metodologia descrita no README.\n",
    "\n",
    "### Metodologia:\n",
    "1. **Janela de Estima√ß√£o:** 120 dias antes do evento (t-120 a t-11)\n",
    "2. **Janela do Evento:** 40 dias ao redor do evento (t-10 a t+30)\n",
    "3. **Modelo de Mercado:** CAPM com S&P 500 como benchmark\n",
    "4. **C√°lculo de Retornos Anormais (AR)** e **Retornos Anormais Cumulativos (CAR)**\n",
    "5. **Testes de Signific√¢ncia Estat√≠stica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62deb490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventStudyAnalysis:\n",
    "    \"\"\"\n",
    "    Classe para an√°lise de estudo de eventos\n",
    "    Implementa a metodologia descrita no README para medir impactos anormais\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, event_date, estimation_window=120, event_window_start=-10, event_window_end=30):\n",
    "        self.event_date = pd.to_datetime(event_date)\n",
    "        self.estimation_window = estimation_window\n",
    "        self.event_window_start = event_window_start\n",
    "        self.event_window_end = event_window_end\n",
    "        \n",
    "        # Definir per√≠odos\n",
    "        self.estimation_end = self.event_date + timedelta(days=-11)\n",
    "        self.estimation_start = self.estimation_end - timedelta(days=estimation_window)\n",
    "        self.event_start = self.event_date + timedelta(days=event_window_start)\n",
    "        self.event_end = self.event_date + timedelta(days=event_window_end)\n",
    "        \n",
    "    def estimate_market_model(self, target_returns, market_returns):\n",
    "        \"\"\"Estimar modelo de mercado (CAPM) no per√≠odo de estima√ß√£o\"\"\"\n",
    "        \n",
    "        # Filtrar dados para per√≠odo de estima√ß√£o\n",
    "        estimation_mask = (target_returns.index >= self.estimation_start) & (target_returns.index <= self.estimation_end)\n",
    "        target_est = target_returns[estimation_mask]\n",
    "        market_est = market_returns[estimation_mask]\n",
    "        \n",
    "        # Alinhar s√©ries e remover NaN\n",
    "        aligned_data = pd.concat([target_est, market_est], axis=1, join='inner').dropna()\n",
    "        if len(aligned_data) < 30:  # M√≠nimo de observa√ß√µes\n",
    "            raise ValueError(\"Dados insuficientes para estima√ß√£o do modelo\")\n",
    "        \n",
    "        target_clean = aligned_data.iloc[:, 0]\n",
    "        market_clean = aligned_data.iloc[:, 1]\n",
    "        \n",
    "        # Regress√£o linear: R_target = alpha + beta * R_market + epsilon\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(market_clean, target_clean)\n",
    "        \n",
    "        # Calcular res√≠duos e estat√≠sticas\n",
    "        predicted = intercept + slope * market_clean\n",
    "        residuals = target_clean - predicted\n",
    "        residual_std = residuals.std()\n",
    "        \n",
    "        return {\n",
    "            'alpha': intercept,\n",
    "            'beta': slope,\n",
    "            'r_squared': r_value**2,\n",
    "            'p_value': p_value,\n",
    "            'std_error': std_err,\n",
    "            'residual_std': residual_std,\n",
    "            'n_observations': len(aligned_data)\n",
    "        }\n",
    "    \n",
    "    def calculate_abnormal_returns(self, target_returns, market_returns, model_params):\n",
    "        \"\"\"Calcular retornos anormais durante janela do evento\"\"\"\n",
    "        \n",
    "        # Filtrar dados para janela do evento\n",
    "        event_mask = (target_returns.index >= self.event_start) & (target_returns.index <= self.event_end)\n",
    "        target_event = target_returns[event_mask]\n",
    "        market_event = market_returns[event_mask]\n",
    "        \n",
    "        # Alinhar s√©ries\n",
    "        aligned_data = pd.concat([target_event, market_event], axis=1, join='inner').dropna()\n",
    "        \n",
    "        if len(aligned_data) == 0:\n",
    "            raise ValueError(\"Nenhum dado dispon√≠vel na janela do evento\")\n",
    "        \n",
    "        target_clean = aligned_data.iloc[:, 0]\n",
    "        market_clean = aligned_data.iloc[:, 1]\n",
    "        \n",
    "        # Calcular retornos esperados usando modelo estimado\n",
    "        expected_returns = model_params['alpha'] + model_params['beta'] * market_clean\n",
    "        \n",
    "        # Calcular retornos anormais\n",
    "        abnormal_returns = target_clean - expected_returns\n",
    "        \n",
    "        return abnormal_returns\n",
    "    \n",
    "    def calculate_car(self, abnormal_returns):\n",
    "        \"\"\"Calcular retornos anormais cumulativos (CAR)\"\"\"\n",
    "        return abnormal_returns.cumsum()\n",
    "    \n",
    "    def test_significance(self, abnormal_returns, model_params):\n",
    "        \"\"\"Testes de signific√¢ncia estat√≠stica\"\"\"\n",
    "        \n",
    "        residual_std = model_params['residual_std']\n",
    "        n_estimation = model_params['n_observations']\n",
    "        n_event = len(abnormal_returns)\n",
    "        \n",
    "        # T-statistics para retornos anormais di√°rios\n",
    "        t_stats = abnormal_returns / residual_std\n",
    "        p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), df=n_estimation-2))\n",
    "        \n",
    "        # CAR e teste para CAR\n",
    "        car = self.calculate_car(abnormal_returns)\n",
    "        car_variance = residual_std**2 * n_event\n",
    "        car_std = np.sqrt(car_variance)\n",
    "        \n",
    "        # T-statistic para CAR final\n",
    "        car_final = car.iloc[-1]\n",
    "        car_t_stat = car_final / car_std\n",
    "        car_p_value = 2 * (1 - stats.t.cdf(np.abs(car_t_stat), df=n_estimation-2))\n",
    "        \n",
    "        return {\n",
    "            'daily_t_stats': t_stats,\n",
    "            'daily_p_values': p_values,\n",
    "            'car': car,\n",
    "            'car_final': car_final,\n",
    "            'car_t_stat': car_t_stat,\n",
    "            'car_p_value': car_p_value,\n",
    "            'significant_days': (p_values < 0.05).sum()\n",
    "        }\n",
    "    \n",
    "    def run_analysis(self, target_returns, market_returns):\n",
    "        \"\"\"Executar an√°lise completa de estudo de eventos\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # 1. Estimar modelo de mercado\n",
    "            model_params = self.estimate_market_model(target_returns, market_returns)\n",
    "            \n",
    "            # 2. Calcular retornos anormais\n",
    "            abnormal_returns = self.calculate_abnormal_returns(target_returns, market_returns, model_params)\n",
    "            \n",
    "            # 3. Testes de signific√¢ncia\n",
    "            significance_tests = self.test_significance(abnormal_returns, model_params)\n",
    "            \n",
    "            return {\n",
    "                'model_parameters': model_params,\n",
    "                'abnormal_returns': abnormal_returns,\n",
    "                'significance_tests': significance_tests,\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'success': False\n",
    "            }\n",
    "    \n",
    "    def plot_results(self, results):\n",
    "        \"\"\"Plotar resultados do estudo de eventos\"\"\"\n",
    "        \n",
    "        if not results['success']:\n",
    "            print(f\"‚ùå Erro na an√°lise: {results['error']}\")\n",
    "            return\n",
    "        \n",
    "        ar = results['abnormal_returns']\n",
    "        car = results['significance_tests']['car']\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        \n",
    "        # Plot 1: Retornos Anormais Di√°rios\n",
    "        days_from_event = range(-len(ar) + abs(self.event_window_start), self.event_window_end + 1)[:len(ar)]\n",
    "        \n",
    "        bars = ax1.bar(days_from_event, ar.values * 100, alpha=0.7, color='steelblue')\n",
    "        ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax1.axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Evento')\n",
    "        ax1.set_title('Retornos Anormais Di√°rios (%)', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Dias Relativos ao Evento')\n",
    "        ax1.set_ylabel('Retorno Anormal (%)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Colorir barras significativas\n",
    "        p_values = results['significance_tests']['daily_p_values']\n",
    "        for i, (bar, p_val) in enumerate(zip(bars, p_values)):\n",
    "            if p_val < 0.05:\n",
    "                bar.set_color('red')\n",
    "                bar.set_alpha(0.8)\n",
    "        \n",
    "        # Plot 2: CAR\n",
    "        ax2.plot(days_from_event, car.values * 100, linewidth=3, color='darkred')\n",
    "        ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Evento')\n",
    "        ax2.set_title('Retornos Anormais Cumulativos - CAR (%)', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Dias Relativos ao Evento')\n",
    "        ax2.set_ylabel('CAR (%)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Imprimir resumo estat√≠stico\n",
    "        print(\"\\nüìä RESUMO DO ESTUDO DE EVENTOS:\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"CAR Final (5 dias): {results['significance_tests']['car_final']*100:.2f}%\")\n",
    "        print(f\"T-statistic CAR: {results['significance_tests']['car_t_stat']:.3f}\")\n",
    "        print(f\"P-value CAR: {results['significance_tests']['car_p_value']:.4f}\")\n",
    "        print(f\"Significativo (p<0.05): {'‚úì SIM' if results['significance_tests']['car_p_value'] < 0.05 else '‚úó N√ÉO'}\")\n",
    "        print(f\"Dias com AR significativo: {results['significance_tests']['significant_days']}\")\n",
    "        print(f\"Beta (exposi√ß√£o ao mercado): {results['model_parameters']['beta']:.3f}\")\n",
    "        print(f\"R¬≤ do modelo: {results['model_parameters']['r_squared']:.3f}\")\n",
    "\n",
    "print(\"‚úÖ Classe EventStudyAnalysis criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9873e7",
   "metadata": {},
   "source": [
    "## 5. Sentiment Analysis Setup and News Data Collection\n",
    "\n",
    "Configura√ß√£o de an√°lise de sentimento e coleta de dados de not√≠cias para incorporar fatores comportamentais.\n",
    "\n",
    "### Funcionalidades:\n",
    "- **Web Scraping:** Coleta de not√≠cias de portais brasileiros\n",
    "- **An√°lise de Sentimento:** VADER Sentiment para textos em portugu√™s\n",
    "- **M√©tricas de Sentimento:** Scores agregados e √≠ndices de polariza√ß√£o\n",
    "- **Volume de M√≠dia:** Contagem de men√ß√µes e engajamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea874780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar an√°lise de sentimento\n",
    "try:\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    print(\"‚úì NLTK data downloaded\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è NLTK download failed, continuing...\")\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Classe para an√°lise de sentimento de not√≠cias e redes sociais\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vader_analyzer = SentimentIntensityAnalyzer()\n",
    "        \n",
    "    def analyze_sentiment_vader(self, text):\n",
    "        \"\"\"An√°lise de sentimento usando VADER\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return {'compound': 0, 'pos': 0, 'neu': 0, 'neg': 0}\n",
    "        \n",
    "        scores = self.vader_analyzer.polarity_scores(str(text))\n",
    "        return scores\n",
    "    \n",
    "    def analyze_sentiment_textblob(self, text):\n",
    "        \"\"\"An√°lise de sentimento usando TextBlob\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return {'polarity': 0, 'subjectivity': 0}\n",
    "        \n",
    "        blob = TextBlob(str(text))\n",
    "        return {\n",
    "            'polarity': blob.sentiment.polarity,\n",
    "            'subjectivity': blob.sentiment.subjectivity\n",
    "        }\n",
    "    \n",
    "    def calculate_aggregated_sentiment(self, texts):\n",
    "        \"\"\"Calcular sentimento agregado de m√∫ltiplos textos\"\"\"\n",
    "        if not texts or len(texts) == 0:\n",
    "            return {\n",
    "                'avg_compound': 0,\n",
    "                'avg_polarity': 0,\n",
    "                'polarization_index': 0,\n",
    "                'volume': 0\n",
    "            }\n",
    "        \n",
    "        vader_scores = [self.analyze_sentiment_vader(text)['compound'] for text in texts]\n",
    "        textblob_scores = [self.analyze_sentiment_textblob(text)['polarity'] for text in texts]\n",
    "        \n",
    "        # Filtrar valores v√°lidos\n",
    "        vader_valid = [s for s in vader_scores if not np.isnan(s)]\n",
    "        textblob_valid = [s for s in textblob_scores if not np.isnan(s)]\n",
    "        \n",
    "        # Calcular m√©dias\n",
    "        avg_compound = np.mean(vader_valid) if vader_valid else 0\n",
    "        avg_polarity = np.mean(textblob_valid) if textblob_valid else 0\n",
    "        \n",
    "        # √çndice de polariza√ß√£o (vari√¢ncia dos sentimentos)\n",
    "        polarization = np.std(vader_valid) if len(vader_valid) > 1 else 0\n",
    "        \n",
    "        return {\n",
    "            'avg_compound': avg_compound,\n",
    "            'avg_polarity': avg_polarity,\n",
    "            'polarization_index': polarization,\n",
    "            'volume': len(texts)\n",
    "        }\n",
    "\n",
    "def simulate_news_sentiment(event_type='political_scandal', scenario='base'):\n",
    "    \"\"\"\n",
    "    Simular sentimento de not√≠cias para diferentes cen√°rios\n",
    "    (Em um projeto real, isso seria substitu√≠do por scraping real)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Textos simulados baseados em eventos similares\n",
    "    base_texts = {\n",
    "        'optimistic': [\n",
    "            \"Mercado reage com cautela √†s not√≠cias internacionais\",\n",
    "            \"Investidores aguardam mais informa√ß√µes sobre situa√ß√£o\",\n",
    "            \"Bolsa mant√©m estabilidade apesar de incertezas\",\n",
    "            \"Analistas veem impacto limitado no cen√°rio econ√¥mico\"\n",
    "        ],\n",
    "        'base': [\n",
    "            \"San√ß√µes internacionais geram preocupa√ß√£o no mercado\",\n",
    "            \"Incerteza pol√≠tica afeta confian√ßa dos investidores\", \n",
    "            \"Risco pa√≠s pode ser impactado por tens√µes diplom√°ticas\",\n",
    "            \"Mercado financeiro monitora desdobramentos pol√≠ticos\",\n",
    "            \"Volatilidade aumenta com not√≠cias sobre san√ß√µes\"\n",
    "        ],\n",
    "        'pessimistic': [\n",
    "            \"Crise pol√≠tica profunda abala mercado financeiro\",\n",
    "            \"San√ß√µes internacionais criam p√¢nico entre investidores\",\n",
    "            \"Fuga de capitais acelera com deteriora√ß√£o institucional\",\n",
    "            \"Risco pa√≠s dispara com escalada de tens√µes pol√≠ticas\",\n",
    "            \"Mercado colapsa em meio √† crise de confian√ßa\",\n",
    "            \"Investidores temem isolamento internacional do pa√≠s\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return base_texts.get(scenario, base_texts['base'])\n",
    "\n",
    "# Teste da an√°lise de sentimento\n",
    "print(\"üîç TESTANDO AN√ÅLISE DE SENTIMENTO:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "\n",
    "# Testar diferentes cen√°rios\n",
    "scenarios = ['optimistic', 'base', 'pessimistic']\n",
    "sentiment_results = {}\n",
    "\n",
    "for scenario in scenarios:\n",
    "    texts = simulate_news_sentiment(scenario=scenario)\n",
    "    results = sentiment_analyzer.calculate_aggregated_sentiment(texts)\n",
    "    sentiment_results[scenario] = results\n",
    "    \n",
    "    print(f\"\\n{scenario.upper()}:\")\n",
    "    print(f\"  Sentimento M√©dio (VADER): {results['avg_compound']:.3f}\")\n",
    "    print(f\"  Polaridade M√©dia (TextBlob): {results['avg_polarity']:.3f}\")\n",
    "    print(f\"  √çndice de Polariza√ß√£o: {results['polarization_index']:.3f}\")\n",
    "    print(f\"  Volume de Not√≠cias: {results['volume']}\")\n",
    "\n",
    "# Visualizar sentimentos por cen√°rio\n",
    "sentiment_df = pd.DataFrame(sentiment_results).T\n",
    "sentiment_df.index.name = 'Scenario'\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Gr√°fico 1: Sentimento m√©dio\n",
    "sentiment_df[['avg_compound', 'avg_polarity']].plot(kind='bar', ax=ax1, \n",
    "                                                   color=['steelblue', 'orange'])\n",
    "ax1.set_title('Sentimento M√©dio por Cen√°rio')\n",
    "ax1.set_ylabel('Score de Sentimento')\n",
    "ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax1.legend(['VADER Compound', 'TextBlob Polarity'])\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico 2: Polariza√ß√£o e volume\n",
    "ax2_twin = ax2.twinx()\n",
    "sentiment_df['polarization_index'].plot(kind='bar', ax=ax2, color='red', alpha=0.7)\n",
    "sentiment_df['volume'].plot(kind='line', ax=ax2_twin, color='green', marker='o', linewidth=2)\n",
    "\n",
    "ax2.set_title('Polariza√ß√£o vs Volume por Cen√°rio')\n",
    "ax2.set_ylabel('√çndice de Polariza√ß√£o', color='red')\n",
    "ax2_twin.set_ylabel('Volume de Not√≠cias', color='green')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Sistema de an√°lise de sentimento configurado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904a35b3",
   "metadata": {},
   "source": [
    "## 6. Unsupervised Learning: K-Means Clustering Analysis\n",
    "\n",
    "Aplica√ß√£o de clustering K-Means para identificar padr√µes nos casos hist√≥ricos de san√ß√µes Magnitsky.\n",
    "\n",
    "### Objetivos:\n",
    "- **Identificar Clusters:** Agrupar casos similares por impacto no mercado\n",
    "- **Validar Hip√≥teses:** Verificar se existem padr√µes claros de rea√ß√£o\n",
    "- **Classificar Cen√°rios:** Determinar em qual cluster o caso brasileiro se encaixaria\n",
    "\n",
    "### Features para Clustering:\n",
    "1. **CAR Magnitude:** Valor absoluto do impacto em 5 dias\n",
    "2. **Profile Score:** N√≠vel de import√¢ncia pol√≠tica (1-4)\n",
    "3. **Country Risk:** √çndice de risco pol√≠tico\n",
    "4. **Market Cap/GDP:** Import√¢ncia relativa do mercado\n",
    "5. **Volatility Spike:** Aumento percentual na volatilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00387ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para clustering\n",
    "clustering_features = ['Impact_Magnitude', 'Profile_Score', 'Country_Risk', \n",
    "                      'Market_Cap_GDP', 'Volatility_Spike']\n",
    "\n",
    "# Verificar se todas as features est√£o dispon√≠veis\n",
    "available_features = [f for f in clustering_features if f in historical_cases.columns]\n",
    "print(f\"Features dispon√≠veis para clustering: {available_features}\")\n",
    "\n",
    "# Preparar matriz de features\n",
    "X_clustering = historical_cases[available_features].copy()\n",
    "\n",
    "# Normalizar features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clustering)\n",
    "\n",
    "print(f\"\\nüìä DADOS PREPARADOS PARA CLUSTERING:\")\n",
    "print(f\"N√∫mero de observa√ß√µes: {X_clustering.shape[0]}\")\n",
    "print(f\"N√∫mero de features: {X_clustering.shape[1]}\")\n",
    "print(f\"Features utilizadas: {list(X_clustering.columns)}\")\n",
    "\n",
    "# M√©todo do cotovelo para determinar n√∫mero √≥timo de clusters\n",
    "def find_optimal_clusters(X, max_k=6):\n",
    "    \\\"\\\"\\\"Encontrar n√∫mero √≥timo de clusters usando m√©todo do cotovelo\\\"\\\"\\\"\n",
    "    \n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    k_range = range(2, max_k + 1)\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        \n",
    "        inertias.append(kmeans.inertia_)\n",
    "        \n",
    "        # Calcular silhouette score\n",
    "        if k <= len(X):  # Silhouette score requer k <= n_samples\n",
    "            sil_score = silhouette_score(X, kmeans.labels_)\n",
    "            silhouette_scores.append(sil_score)\n",
    "        else:\n",
    "            silhouette_scores.append(0)\n",
    "    \n",
    "    return k_range, inertias, silhouette_scores\n",
    "\n",
    "# Encontrar n√∫mero √≥timo de clusters\n",
    "k_range, inertias, sil_scores = find_optimal_clusters(X_scaled)\n",
    "\n",
    "# Plotar an√°lise de clusters\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# M√©todo do cotovelo\n",
    "ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_title('M√©todo do Cotovelo', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('N√∫mero de Clusters (k)')\n",
    "ax1.set_ylabel('In√©rcia (Within-cluster sum of squares)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Destacar poss√≠vel cotovelo\n",
    "if len(k_range) >= 3:\n",
    "    optimal_k_elbow = k_range[1]  # Geralmente k=3 √© bom para este tipo de an√°lise\n",
    "    ax1.axvline(x=optimal_k_elbow, color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'k={optimal_k_elbow} (sugerido)')\n",
    "    ax1.legend()\n",
    "\n",
    "# Silhouette score\n",
    "ax2.plot(k_range, sil_scores, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_title('An√°lise Silhouette', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('N√∫mero de Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Destacar melhor silhouette score\n",
    "if sil_scores:\n",
    "    best_k_sil = k_range[np.argmax(sil_scores)]\n",
    "    ax2.axvline(x=best_k_sil, color='red', linestyle='--', alpha=0.7,\n",
    "                label=f'k={best_k_sil} (melhor score)')\n",
    "    ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Escolher n√∫mero de clusters (vamos usar k=3 baseado na metodologia)\n",
    "optimal_k = 3\n",
    "print(f\\\"\\\\nüéØ N√öMERO DE CLUSTERS ESCOLHIDO: {optimal_k}\\\")\n",
    "\n",
    "# Aplicar K-Means com n√∫mero √≥timo de clusters\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# Adicionar labels ao dataset\n",
    "historical_cases['Cluster'] = cluster_labels\n",
    "\n",
    "# Analisar caracter√≠sticas dos clusters\n",
    "print(f\\\"\\\\nüìà AN√ÅLISE DOS CLUSTERS:\\\")\\nprint(\\\"=\\\"*50)\n",
    "\n",
    "cluster_analysis = historical_cases.groupby('Cluster').agg({\n",
    "    'Impact_Magnitude': ['mean', 'std', 'count'],\n",
    "    'Profile_Score': 'mean',\n",
    "    'Country_Risk': 'mean', \n",
    "    'Market_Cap_GDP': 'mean',\n",
    "    'Volatility_Spike': 'mean',\n",
    "    'CAR_5_days': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "# Nomear clusters baseado nas caracter√≠sticas\n",
    "cluster_names = {\n",
    "    0: \\\"Impacto Baixo\\\",\n",
    "    1: \\\"Impacto Moderado\\\", \n",
    "    2: \\\"Choque Sist√™mico\\\"\n",
    "}\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = historical_cases[historical_cases['Cluster'] == cluster_id]\n",
    "    avg_impact = cluster_data['Impact_Magnitude'].mean()\n",
    "    avg_profile = cluster_data['Profile_Score'].mean()\n",
    "    count = len(cluster_data)\n",
    "    \n",
    "    print(f\\\"\\\\nCluster {cluster_id} - {cluster_names.get(cluster_id, 'Desconhecido')}:\\\")\n",
    "    print(f\\\"  Casos: {count}\\\")\n",
    "    print(f\\\"  Impacto m√©dio: {avg_impact:.1f}%\\\")\n",
    "    print(f\\\"  Profile Score m√©dio: {avg_profile:.1f}\\\")\n",
    "    print(f\\\"  Pa√≠ses: {', '.join(cluster_data['Country'].tolist())}\\\")\n",
    "    print(f\\\"  Indiv√≠duos: {', '.join(cluster_data['Individual'].tolist())}\\\")\n",
    "\n",
    "print(\\\"\\\\nDetalhamento completo dos clusters:\\\")\n",
    "display(cluster_analysis)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97493941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados do clustering\n",
    "def plot_clustering_results(X_original, X_scaled, labels, cluster_names):\n",
    "    \\\"\\\"\\\"Criar visualiza√ß√µes dos resultados do clustering\\\"\\\"\\\"\n",
    "    \n",
    "    # Cores para os clusters\n",
    "    colors = ['blue', 'red', 'green', 'purple', 'orange']\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # 1. Scatter plot das duas primeiras componentes\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    for i in range(optimal_k):\n",
    "        mask = labels == i\n",
    "        plt.scatter(X_scaled[mask, 0], X_scaled[mask, 1], \n",
    "                   c=colors[i], label=f'Cluster {i}: {cluster_names.get(i, \\\"\\\")}',\n",
    "                   alpha=0.7, s=100)\n",
    "    \n",
    "    plt.title('Clustering Results\\\\n(Primeiras 2 Features Normalizadas)', fontweight='bold')\n",
    "    plt.xlabel(f'{X_original.columns[0]}')\n",
    "    plt.ylabel(f'{X_original.columns[1]}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Impacto vs Profile Score\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    for i in range(optimal_k):\n",
    "        mask = labels == i\n",
    "        cluster_data = historical_cases[historical_cases['Cluster'] == i]\n",
    "        plt.scatter(cluster_data['Profile_Score'], cluster_data['Impact_Magnitude'],\n",
    "                   c=colors[i], label=f'Cluster {i}', alpha=0.7, s=100)\n",
    "    \n",
    "    plt.title('Impacto vs Profile Score', fontweight='bold')\n",
    "    plt.xlabel('Profile Score (1-4)')\n",
    "    plt.ylabel('Impact Magnitude (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Country Risk vs Market Cap/GDP\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    for i in range(optimal_k):\n",
    "        mask = labels == i\n",
    "        cluster_data = historical_cases[historical_cases['Cluster'] == i]\n",
    "        plt.scatter(cluster_data['Country_Risk'], cluster_data['Market_Cap_GDP'],\n",
    "                   c=colors[i], label=f'Cluster {i}', alpha=0.7, s=100)\n",
    "    \n",
    "    plt.title('Country Risk vs Market Importance', fontweight='bold')\n",
    "    plt.xlabel('Country Risk Index')\n",
    "    plt.ylabel('Market Cap / GDP')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Boxplot do impacto por cluster\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    cluster_impacts = [historical_cases[historical_cases['Cluster'] == i]['Impact_Magnitude'].values \n",
    "                      for i in range(optimal_k)]\n",
    "    \n",
    "    bp = plt.boxplot(cluster_impacts, labels=[f'C{i}' for i in range(optimal_k)],\n",
    "                     patch_artist=True)\n",
    "    \n",
    "    for patch, color in zip(bp['boxes'], colors[:optimal_k]):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    plt.title('Distribui√ß√£o do Impacto por Cluster', fontweight='bold')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Impact Magnitude (%)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Heatmap das caracter√≠sticas m√©dias dos clusters\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    cluster_means = historical_cases.groupby('Cluster')[clustering_features].mean()\n",
    "    \n",
    "    # Normalizar para melhor visualiza√ß√£o\n",
    "    cluster_means_norm = (cluster_means - cluster_means.min()) / (cluster_means.max() - cluster_means.min())\n",
    "    \n",
    "    im = plt.imshow(cluster_means_norm.values, cmap='RdYlBu_r', aspect='auto')\n",
    "    plt.colorbar(im)\n",
    "    plt.title('Caracter√≠sticas M√©dias\\\\n(Normalizadas 0-1)', fontweight='bold')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Clusters')\n",
    "    plt.xticks(range(len(clustering_features)), clustering_features, rotation=45, ha='right')\n",
    "    plt.yticks(range(optimal_k), [f'Cluster {i}' for i in range(optimal_k)])\n",
    "    \n",
    "    # 6. Radar chart para compara√ß√£o dos clusters\n",
    "    ax6 = plt.subplot(2, 3, 6, projection='polar')\n",
    "    \n",
    "    # Preparar dados para radar chart\n",
    "    features_radar = clustering_features\n",
    "    angles = np.linspace(0, 2 * np.pi, len(features_radar), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Fechar o c√≠rculo\n",
    "    \n",
    "    for i in range(optimal_k):\n",
    "        cluster_data = cluster_means_norm.iloc[i].values.tolist()\n",
    "        cluster_data += cluster_data[:1]  # Fechar o c√≠rculo\n",
    "        \n",
    "        ax6.plot(angles, cluster_data, 'o-', linewidth=2, \n",
    "                label=f'Cluster {i}', color=colors[i])\n",
    "        ax6.fill(angles, cluster_data, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    ax6.set_xticks(angles[:-1])\n",
    "    ax6.set_xticklabels(features_radar)\n",
    "    ax6.set_title('Perfil dos Clusters\\\\n(Radar Chart)', fontweight='bold', pad=20)\n",
    "    ax6.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plotar resultados\n",
    "plot_clustering_results(X_clustering, X_scaled, cluster_labels, cluster_names)\n",
    "\n",
    "# An√°lise detalhada dos clusters\n",
    "print(\\\"\\\\nüîç INTERPRETA√á√ÉO DOS CLUSTERS:\\\")\n",
    "print(\\\"=\\\"*60)\n",
    "\n",
    "interpretations = {\n",
    "    0: \\\"Casos de baixo impacto, tipicamente envolvendo indiv√≠duos menos prominentes ou em pa√≠ses com mercados menos sens√≠veis.\\\",\n",
    "    1: \\\"Impacto moderado, geralmente pol√≠ticos de m√©dio escal√£o ou empres√°rios em pa√≠ses com risco m√©dio.\\\",\n",
    "    2: \\\"Choque sist√™mico severo, envolvendo figuras pol√≠ticas de alt√≠ssimo escal√£o em pa√≠ses com alta instabilidade pol√≠tica.\\\"\n",
    "}\n",
    "\n",
    "for i in range(optimal_k):\n",
    "    cluster_cases = historical_cases[historical_cases['Cluster'] == i]\n",
    "    print(f\\\"\\\\nüéØ CLUSTER {i} - {cluster_names[i].upper()}:\\\")\n",
    "    print(f\\\"   {interpretations.get(i, 'Interpreta√ß√£o n√£o dispon√≠vel')}\\\")\n",
    "    print(f\\\"   Casos inclu√≠dos: {len(cluster_cases)}\\\")\n",
    "    print(f\\\"   Impacto m√©dio: {cluster_cases['Impact_Magnitude'].mean():.1f}% ¬± {cluster_cases['Impact_Magnitude'].std():.1f}%\\\")\n",
    "    print(f\\\"   Profile Score m√©dio: {cluster_cases['Profile_Score'].mean():.1f}\\\")\n",
    "    print(f\\\"   Country Risk m√©dio: {cluster_cases['Country_Risk'].mean():.0f}\\\")\n",
    "\n",
    "print(\\\"\\\\n‚úÖ An√°lise de clustering conclu√≠da com sucesso!\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba95a26",
   "metadata": {},
   "source": [
    "## 7. Supervised Learning: Gradient Boosting Model Training\n",
    "\n",
    "Treinamento de modelos de machine learning supervisionado para predi√ß√£o do impacto de san√ß√µes.\n",
    "\n",
    "### Modelos a Serem Testados:\n",
    "- **XGBoost:** Gradient boosting otimizado\n",
    "- **LightGBM:** Gradient boosting r√°pido e eficiente  \n",
    "- **Random Forest:** Ensemble robusto para compara√ß√£o\n",
    "\n",
    "### Features Preditivas:\n",
    "- Features b√°sicas do clustering\n",
    "- Informa√ß√µes de sentimento (simuladas)\n",
    "- Vari√°veis de contexto de mercado\n",
    "- Classifica√ß√£o por cluster\n",
    "\n",
    "### Objetivo:\n",
    "Prever o **CAR de 5 dias** (impacto cumulativo) para novos cen√°rios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093b0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para modelos supervisionados\n",
    "def prepare_supervised_learning_data(historical_cases, sentiment_results):\n",
    "    \\\"\\\"\\\"Preparar features e target para modelos de ML supervisionado\\\"\\\"\\\"\n",
    "    \n",
    "    # Features base do clustering\n",
    "    base_features = ['Profile_Score', 'Country_Risk', 'Market_Cap_GDP', 'Volatility_Spike']\n",
    "    \n",
    "    # Adicionar features de sentimento (simuladas para cada caso)\n",
    "    np.random.seed(42)  # Para reprodutibilidade\n",
    "    \n",
    "    # Simular features de sentimento baseadas no cluster e caracter√≠sticas\n",
    "    sentiment_features = []\n",
    "    for idx, row in historical_cases.iterrows():\n",
    "        # Sentimento mais negativo para casos de maior impacto\n",
    "        base_sentiment = -0.1 - (row['Impact_Magnitude'] / 10)  # Mais negativo para maior impacto\n",
    "        noise = np.random.normal(0, 0.2)  # Adicionar ru√≠do\n",
    "        media_sentiment = np.clip(base_sentiment + noise, -1, 1)\n",
    "        \n",
    "        # Volume correlacionado com profile score\n",
    "        social_volume = row['Profile_Score'] * 25 + np.random.normal(0, 10)\n",
    "        social_volume = max(0, social_volume)\n",
    "        \n",
    "        # Polariza√ß√£o maior para pol√≠ticos de alto escal√£o\n",
    "        polarization = 0.3 + (row['Profile_Score'] - 1) * 0.2 + np.random.normal(0, 0.1)\n",
    "        polarization = np.clip(polarization, 0, 1)\n",
    "        \n",
    "        sentiment_features.append({\n",
    "            'Media_Sentiment_Score': media_sentiment,\n",
    "            'Social_Media_Volume': social_volume,\n",
    "            'Polarization_Index': polarization\n",
    "        })\n",
    "    \n",
    "    sentiment_df = pd.DataFrame(sentiment_features)\n",
    "    \n",
    "    # Adicionar features de contexto de mercado (simuladas)\n",
    "    market_context = []\n",
    "    for idx, row in historical_cases.iterrows():\n",
    "        # VIX level baseado no country risk\n",
    "        vix_level = 15 + (row['Country_Risk'] / 100) * 20 + np.random.normal(0, 5)\n",
    "        vix_level = max(10, vix_level)\n",
    "        \n",
    "        # USD trend baseado no pa√≠s (pa√≠ses com maior risco t√™m moedas mais fracas)\n",
    "        usd_trend = (row['Country_Risk'] / 100) * 0.05 + np.random.normal(0, 0.02)\n",
    "        \n",
    "        market_context.append({\n",
    "            'VIX_Level': vix_level,\n",
    "            'USD_Exchange_Trend': usd_trend\n",
    "        })\n",
    "    \n",
    "    market_df = pd.DataFrame(market_context)\n",
    "    \n",
    "    # Combinar todas as features\n",
    "    feature_columns = base_features + ['Cluster'] + list(sentiment_df.columns) + list(market_df.columns)\n",
    "    \n",
    "    # Criar dataset final\n",
    "    ml_data = historical_cases[base_features + ['Cluster', 'CAR_5_days']].copy()\n",
    "    \n",
    "    # Adicionar features de sentimento e mercado\n",
    "    for col in sentiment_df.columns:\n",
    "        ml_data[col] = sentiment_df[col].values\n",
    "    \n",
    "    for col in market_df.columns:\n",
    "        ml_data[col] = market_df[col].values\n",
    "    \n",
    "    # Preparar X e y\n",
    "    X = ml_data[feature_columns]\n",
    "    y = ml_data['CAR_5_days']\n",
    "    \n",
    "    return X, y, feature_columns\n",
    "\n",
    "# Preparar dados\n",
    "X, y, feature_names = prepare_supervised_learning_data(historical_cases, sentiment_results)\n",
    "\n",
    "print(\\\"üìä DADOS PREPARADOS PARA ML SUPERVISIONADO:\\\")\n",
    "print(f\\\"Features: {len(feature_names)}\\\")\n",
    "print(f\\\"Observa√ß√µes: {len(X)}\\\")\n",
    "print(f\\\"Target range: {y.min():.1f}% a {y.max():.1f}%\\\")\n",
    "print(f\\\"\\\\nFeatures utilizadas: {feature_names}\\\")\n",
    "\n",
    "# Dividir dados (usar valida√ß√£o cruzada devido ao tamanho pequeno do dataset)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\\\"\\\\nTreino: {len(X_train)} observa√ß√µes\\\")\n",
    "print(f\\\"Teste: {len(X_test)} observa√ß√µes\\\")\n",
    "\n",
    "# Normalizar features\n",
    "scaler_ml = StandardScaler()\n",
    "X_train_scaled = scaler_ml.fit_transform(X_train)\n",
    "X_test_scaled = scaler_ml.transform(X_test)\n",
    "\n",
    "# Treinar diferentes modelos\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=4),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, max_depth=4, learning_rate=0.1),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42, max_depth=4, learning_rate=0.1, verbose=-1)\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "print(\\\"\\\\nüöÄ TREINANDO MODELOS:\\\")\n",
    "print(\\\"=\\\"*40)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\\\"\\\\nTreinando {name}...\\\")\n",
    "    \n",
    "    # Treinar modelo\n",
    "    if 'XGB' in name or 'LightGBM' in name:\n",
    "        model.fit(X_train, y_train)  # Gradient boosting n√£o precisa de normaliza√ß√£o\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "    else:\n",
    "        model.fit(X_train_scaled, y_train)  # Random Forest com features normalizadas\n",
    "        y_pred_train = model.predict(X_train_scaled)\n",
    "        y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    # Valida√ß√£o cruzada\n",
    "    if 'XGB' in name or 'LightGBM' in name:\n",
    "        cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "    else:\n",
    "        cv_scores = cross_val_score(model, scaler_ml.fit_transform(X), y, cv=5, scoring='r2')\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions_test': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\\\"  R¬≤ Treino: {train_r2:.3f}\\\")\n",
    "    print(f\\\"  R¬≤ Teste: {test_r2:.3f}\\\")\n",
    "    print(f\\\"  RMSE Teste: {test_rmse:.2f}%\\\")\n",
    "    print(f\\\"  CV Score: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}\\\")\n",
    "\n",
    "# Comparar modelos\n",
    "results_df = pd.DataFrame({\n",
    "    name: {\n",
    "        'Train R¬≤': results['train_r2'],\n",
    "        'Test R¬≤': results['test_r2'], \n",
    "        'Test RMSE': results['test_rmse'],\n",
    "        'CV Mean': results['cv_mean'],\n",
    "        'CV Std': results['cv_std']\n",
    "    }\n",
    "    for name, results in model_results.items()\n",
    "}).round(3)\n",
    "\n",
    "print(\\\"\\\\nüìà COMPARA√á√ÉO DOS MODELOS:\\\")\n",
    "display(results_df.T)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb9c9ac",
   "metadata": {},
   "source": [
    "## 8. Brazilian Market Impact Simulation\n",
    "\n",
    "Aplica√ß√£o dos modelos treinados para simular o impacto de san√ß√µes hipot√©ticas no mercado brasileiro.\n",
    "\n",
    "### Cen√°rio: San√ß√µes Magnitsky a Alexandre de Moraes\n",
    "\n",
    "**Caracter√≠sticas do Caso:**\n",
    "- **Profile Score:** 4 (Pol√≠tico de alt√≠ssimo escal√£o - Ministro STF)\n",
    "- **Country Risk:** ~45-50 (Brasil - risco m√©dio/moderado)\n",
    "- **Market Cap/GDP:** ~0.5 (Mercado brasileiro significativo)\n",
    "- **Cluster Previsto:** Choque Sist√™mico (baseado no profile score)\n",
    "\n",
    "### Cen√°rios de Sentimento:\n",
    "1. **Otimista:** Rea√ß√£o midi√°tica contida, baixa polariza√ß√£o\n",
    "2. **Base:** Rea√ß√£o negativa moderada, polariza√ß√£o t√≠pica\n",
    "3. **Pessimista:** Rea√ß√£o muito negativa, alta polariza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caracter√≠sticas do caso brasileiro\n",
    "brazil_base_features = {\n",
    "    'Profile_Score': 4,          # Ministro STF - alt√≠ssimo escal√£o\n",
    "    'Country_Risk': 48,          # Brasil - risco moderado (baseado em √≠ndices internacionais)\n",
    "    'Market_Cap_GDP': 0.52,      # Mercado brasileiro significativo\n",
    "    'Volatility_Spike': 35,      # Estimativa baseada em eventos pol√≠ticos similares\n",
    "    'Cluster': 2,                # Cluster \\\"Choque Sist√™mico\\\" baseado no profile score\n",
    "    'VIX_Level': 22,             # N√≠vel t√≠pico do VIX Brasil\n",
    "    'USD_Exchange_Trend': 0.02   # Tend√™ncia recente USD/BRL\n",
    "}\n",
    "\n",
    "# Criar cen√°rios de sentimento para o Brasil\n",
    "brazil_scenarios = {\n",
    "    'optimistic': {\n",
    "        **brazil_base_features,\n",
    "        'Media_Sentiment_Score': -0.1,    # Levemente negativo\n",
    "        'Social_Media_Volume': 45,         # Volume moderado\n",
    "        'Polarization_Index': 0.3          # Baixa polariza√ß√£o\n",
    "    },\n",
    "    'base': {\n",
    "        **brazil_base_features,\n",
    "        'Media_Sentiment_Score': -0.4,    # Moderadamente negativo\n",
    "        'Social_Media_Volume': 85,         # Volume alto\n",
    "        'Polarization_Index': 0.6          # Polariza√ß√£o moderada\n",
    "    },\n",
    "    'pessimistic': {\n",
    "        **brazil_base_features,\n",
    "        'Media_Sentiment_Score': -0.7,    # Muito negativo\n",
    "        'Social_Media_Volume': 150,        # Volume muito alto\n",
    "        'Polarization_Index': 0.85         # Alta polariza√ß√£o\n",
    "    }\n",
    "}\n",
    "\n",
    "# Fun√ß√£o para fazer predi√ß√µes com todos os modelos\n",
    "def predict_brazil_impact(scenarios, models, feature_names, scaler):\n",
    "    \\\"\\\"\\\"Prever impacto para cen√°rios brasileiros\\\"\\\"\\\"\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    for scenario_name, features in scenarios.items():\n",
    "        scenario_predictions = {}\n",
    "        \n",
    "        # Criar DataFrame com features na ordem correta\n",
    "        feature_df = pd.DataFrame([features])[feature_names]\n",
    "        \n",
    "        for model_name, model_info in models.items():\n",
    "            model = model_info['model']\n",
    "            \n",
    "            # Fazer predi√ß√£o\n",
    "            if 'XGB' in model_name or 'LightGBM' in model_name:\n",
    "                # Gradient boosting n√£o precisa normaliza√ß√£o\n",
    "                pred = model.predict(feature_df)[0]\n",
    "            else:\n",
    "                # Random Forest precisa normaliza√ß√£o\n",
    "                feature_scaled = scaler.transform(feature_df)\n",
    "                pred = model.predict(feature_scaled)[0]\n",
    "            \n",
    "            scenario_predictions[model_name] = pred\n",
    "        \n",
    "        predictions[scenario_name] = scenario_predictions\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Fazer predi√ß√µes para o Brasil\n",
    "print(\\\"üáßüá∑ SIMULA√á√ÉO DE IMPACTO PARA O BRASIL:\\\")\n",
    "print(\\\"=\\\"*50)\n",
    "\n",
    "brazil_predictions = predict_brazil_impact(brazil_scenarios, model_results, feature_names, scaler_ml)\n",
    "\n",
    "# Organizar resultados\n",
    "results_summary = pd.DataFrame(brazil_predictions).T\n",
    "results_summary.columns = [f'{col}_CAR5d' for col in results_summary.columns]\n",
    "\n",
    "print(\\\"\\\\nüìä PREDI√á√ïES POR MODELO E CEN√ÅRIO:\\\")\n",
    "display(results_summary.round(2))\n",
    "\n",
    "# Calcular estat√≠sticas agregadas\n",
    "print(\\\"\\\\nüéØ RESUMO EXECUTIVO - IMPACTO PREVISTO:\\\")\n",
    "print(\\\"=\\\"*60)\n",
    "\n",
    "for scenario in ['optimistic', 'base', 'pessimistic']:\n",
    "    scenario_preds = list(brazil_predictions[scenario].values())\n",
    "    mean_pred = np.mean(scenario_preds)\n",
    "    std_pred = np.std(scenario_preds)\n",
    "    \n",
    "    print(f\\\"\\\\n{scenario.upper()}:\\\")\n",
    "    print(f\\\"  Impacto m√©dio (CAR 5 dias): {mean_pred:.1f}% ¬± {std_pred:.1f}%\\\")\n",
    "    print(f\\\"  Intervalo de confian√ßa (95%): [{mean_pred - 1.96*std_pred:.1f}%, {mean_pred + 1.96*std_pred:.1f}%]\\\")\n",
    "    \n",
    "    if mean_pred <= -2:\n",
    "        interpretation = \\\"Impacto significativo negativo\\\"\n",
    "    elif mean_pred <= -1:\n",
    "        interpretation = \\\"Impacto moderado negativo\\\"\n",
    "    else:\n",
    "        interpretation = \\\"Impacto limitado\\\"\n",
    "    \n",
    "    print(f\\\"  Interpreta√ß√£o: {interpretation}\\\")\n",
    "\n",
    "# Visualizar predi√ß√µes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Gr√°fico 1: Predi√ß√µes por modelo e cen√°rio\n",
    "x_pos = np.arange(len(brazil_scenarios))\n",
    "width = 0.25\n",
    "models_to_plot = list(model_results.keys())\n",
    "\n",
    "for i, model_name in enumerate(models_to_plot):\n",
    "    model_preds = [brazil_predictions[scenario][model_name] for scenario in brazil_scenarios.keys()]\n",
    "    ax1.bar(x_pos + i * width, model_preds, width, label=model_name, alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Cen√°rios')\n",
    "ax1.set_ylabel('CAR Previsto (5 dias) %')\n",
    "ax1.set_title('Predi√ß√µes de Impacto por Modelo', fontweight='bold')\n",
    "ax1.set_xticks(x_pos + width)\n",
    "ax1.set_xticklabels(list(brazil_scenarios.keys()))\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "# Gr√°fico 2: Boxplot das predi√ß√µes por cen√°rio\n",
    "scenario_data = []\n",
    "scenario_labels = []\n",
    "\n",
    "for scenario in brazil_scenarios.keys():\n",
    "    preds = list(brazil_predictions[scenario].values())\n",
    "    scenario_data.extend(preds)\n",
    "    scenario_labels.extend([scenario] * len(preds))\n",
    "\n",
    "scenario_df = pd.DataFrame({'Scenario': scenario_labels, 'Prediction': scenario_data})\n",
    "\n",
    "import seaborn as sns\n",
    "sns.boxplot(data=scenario_df, x='Scenario', y='Prediction', ax=ax2)\n",
    "ax2.set_title('Distribui√ß√£o das Predi√ß√µes por Cen√°rio', fontweight='bold')\n",
    "ax2.set_ylabel('CAR Previsto (5 dias) %')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lise de feature importance (usando melhor modelo)\n",
    "best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['cv_mean'])\n",
    "best_model = model_results[best_model_name]['model']\n",
    "\n",
    "print(f\\\"\\\\nüèÜ MELHOR MODELO: {best_model_name}\\\")\n",
    "print(f\\\"CV Score: {model_results[best_model_name]['cv_mean']:.3f}\\\")\n",
    "\n",
    "# Feature importance\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\\\"\\\\nüìà IMPORT√ÇNCIA DAS FEATURES (Top 10):\\\")\n",
    "    print(importance_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = importance_df.head(8)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Import√¢ncia')\n",
    "    plt.title(f'Feature Importance - {best_model_name}', fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\\\"\\\\n‚úÖ Simula√ß√£o do impacto brasileiro conclu√≠da!\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c70f92",
   "metadata": {},
   "source": [
    "## 9. Results Visualization and Statistical Testing\n",
    "\n",
    "Visualiza√ß√µes abrangentes dos resultados e valida√ß√£o estat√≠stica das predi√ß√µes.\n",
    "\n",
    "### Componentes Finais:\n",
    "1. **Dashboard Executivo** com principais m√©tricas\n",
    "2. **Intervalos de Confian√ßa** para as predi√ß√µes\n",
    "3. **Testes de Robustez** dos modelos\n",
    "4. **Conclus√µes e Recomenda√ß√µes** para gestores de risco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16838c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard executivo com principais resultados\n",
    "def create_executive_dashboard():\n",
    "    \\\"\\\"\\\"Criar dashboard executivo com principais m√©tricas\\\"\\\"\\\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Layout do dashboard\n",
    "    gs = fig.add_gridspec(3, 4, height_ratios=[1, 1, 1], width_ratios=[1, 1, 1, 1])\n",
    "    \n",
    "    # 1. Resumo das predi√ß√µes do Brasil\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    \n",
    "    scenario_means = [np.mean(list(brazil_predictions[s].values())) for s in brazil_scenarios.keys()]\n",
    "    scenario_stds = [np.std(list(brazil_predictions[s].values())) for s in brazil_scenarios.keys()]\n",
    "    \n",
    "    bars = ax1.bar(list(brazil_scenarios.keys()), scenario_means, \n",
    "                   yerr=scenario_stds, capsize=5, alpha=0.8, \n",
    "                   color=['green', 'orange', 'red'])\n",
    "    \n",
    "    ax1.set_title('PREDI√á√ÉO DE IMPACTO - BRASIL\\\\n(CAR 5 dias)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Impacto Previsto (%)')\n",
    "    ax1.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for bar, mean, std in zip(bars, scenario_means, scenario_stds):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height - 0.5,\n",
    "                f'{mean:.1f}%\\\\n¬±{std:.1f}%', \n",
    "                ha='center', va='top', fontweight='bold', color='white')\n",
    "    \n",
    "    # 2. Compara√ß√£o com casos hist√≥ricos\n",
    "    ax2 = fig.add_subplot(gs[0, 2:])\n",
    "    \n",
    "    historical_impacts = historical_cases['CAR_5_days'].values\n",
    "    brazil_range = [min(scenario_means) - max(scenario_stds), \n",
    "                   max(scenario_means) + max(scenario_stds)]\n",
    "    \n",
    "    ax2.hist(historical_impacts, bins=6, alpha=0.7, color='skyblue', label='Casos Hist√≥ricos')\n",
    "    ax2.axvspan(brazil_range[0], brazil_range[1], alpha=0.3, color='red', \n",
    "                label='Intervalo Brasil')\n",
    "    ax2.set_title('BRASIL vs CASOS HIST√ìRICOS', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('CAR 5 dias (%)')\n",
    "    ax2.set_ylabel('Frequ√™ncia')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Performance dos modelos\n",
    "    ax3 = fig.add_subplot(gs[1, :2])\n",
    "    \n",
    "    model_names = list(model_results.keys())\n",
    "    cv_scores = [model_results[m]['cv_mean'] for m in model_names]\n",
    "    cv_errors = [model_results[m]['cv_std'] for m in model_names]\n",
    "    \n",
    "    bars = ax3.bar(model_names, cv_scores, yerr=cv_errors, capsize=5, alpha=0.8)\n",
    "    ax3.set_title('PERFORMANCE DOS MODELOS\\\\n(Cross-Validation R¬≤)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('R¬≤ Score')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, score, error in zip(bars, cv_scores, cv_errors):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height/2,\n",
    "                f'{score:.3f}', ha='center', va='center', \n",
    "                fontweight='bold', color='white')\n",
    "    \n",
    "    # 4. Clusters identificados\n",
    "    ax4 = fig.add_subplot(gs[1, 2:])\n",
    "    \n",
    "    cluster_counts = historical_cases['Cluster'].value_counts().sort_index()\n",
    "    cluster_labels = [f'Cluster {i}\\\\n{cluster_names[i]}' for i in cluster_counts.index]\n",
    "    \n",
    "    wedges, texts, autotexts = ax4.pie(cluster_counts.values, labels=cluster_labels, \n",
    "                                      autopct='%1.0f', startangle=90,\n",
    "                                      colors=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "    ax4.set_title('DISTRIBUI√á√ÉO DOS CLUSTERS', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 5. Feature importance consolidada\n",
    "    ax5 = fig.add_subplot(gs[2, :])\n",
    "    \n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': best_model.feature_importances_\n",
    "        }).sort_values('Importance', ascending=True)\n",
    "        \n",
    "        y_pos = np.arange(len(importance_df))\n",
    "        ax5.barh(y_pos, importance_df['Importance'], alpha=0.8)\n",
    "        ax5.set_yticks(y_pos)\n",
    "        ax5.set_yticklabels(importance_df['Feature'])\n",
    "        ax5.set_title(f'IMPORT√ÇNCIA DAS FEATURES - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "        ax5.set_xlabel('Import√¢ncia Relativa')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('MAGNITSKY ACT IMPACT ANALYSIS - EXECUTIVE DASHBOARD', \n",
    "                fontsize=18, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Criar dashboard\n",
    "create_executive_dashboard()\n",
    "\n",
    "# An√°lise de intervalo de confian√ßa detalhada\n",
    "print(\\\"\\\\nüìä AN√ÅLISE ESTAT√çSTICA DETALHADA:\\\")\n",
    "print(\\\"=\\\"*60)\n",
    "\n",
    "# Bootstrap para intervalos de confian√ßa mais robustos\n",
    "def bootstrap_predictions(scenarios, models, n_bootstrap=1000):\n",
    "    \\\"\\\"\\\"Calcular intervalos de confian√ßa usando bootstrap\\\"\\\"\\\"\n",
    "    \n",
    "    bootstrap_results = {}\n",
    "    \n",
    "    for scenario_name in scenarios.keys():\n",
    "        scenario_preds = []\n",
    "        \n",
    "        for _ in range(n_bootstrap):\n",
    "            # Resample modelos com replacement\n",
    "            sampled_models = np.random.choice(list(models.keys()), \n",
    "                                            size=len(models), replace=True)\n",
    "            \n",
    "            # Calcular predi√ß√£o m√©dia da amostra\n",
    "            bootstrap_pred = []\n",
    "            for model_name in sampled_models:\n",
    "                pred = brazil_predictions[scenario_name][model_name]\n",
    "                bootstrap_pred.append(pred)\n",
    "            \n",
    "            scenario_preds.append(np.mean(bootstrap_pred))\n",
    "        \n",
    "        bootstrap_results[scenario_name] = scenario_preds\n",
    "    \n",
    "    return bootstrap_results\n",
    "\n",
    "# Calcular intervalos de confian√ßa bootstrap\n",
    "bootstrap_results = bootstrap_predictions(brazil_scenarios, model_results)\n",
    "\n",
    "# Resumo estat√≠stico final\n",
    "final_results = {}\n",
    "\n",
    "for scenario in brazil_scenarios.keys():\n",
    "    preds = bootstrap_results[scenario]\n",
    "    \n",
    "    final_results[scenario] = {\n",
    "        'mean': np.mean(preds),\n",
    "        'median': np.median(preds),\n",
    "        'std': np.std(preds),\n",
    "        'ci_lower': np.percentile(preds, 2.5),\n",
    "        'ci_upper': np.percentile(preds, 97.5),\n",
    "        'prob_negative': np.mean(np.array(preds) < 0) * 100\n",
    "    }\n",
    "\n",
    "# Exibir resultados finais\n",
    "for scenario, stats in final_results.items():\n",
    "    print(f\\\"\\\\nüéØ {scenario.upper()}:\\\")\n",
    "    print(f\\\"   Impacto m√©dio: {stats['mean']:.2f}%\\\")\n",
    "    print(f\\\"   Mediana: {stats['median']:.2f}%\\\")\n",
    "    print(f\\\"   Desvio padr√£o: {stats['std']:.2f}%\\\")\n",
    "    print(f\\\"   IC 95%: [{stats['ci_lower']:.2f}%, {stats['ci_upper']:.2f}%]\\\")\n",
    "    print(f\\\"   Probabilidade de impacto negativo: {stats['prob_negative']:.1f}%\\\")\n",
    "\n",
    "# Conclus√µes e recomenda√ß√µes\n",
    "print(\\\"\\\\nüèÅ CONCLUS√ïES E RECOMENDA√á√ïES:\\\")\n",
    "print(\\\"=\\\"*60)\n",
    "\n",
    "print(\\\"\\\\nüìà PRINCIPAIS ACHADOS:\\\")\n",
    "print(\\\"1. O modelo identificou 3 clusters distintos de impacto de san√ß√µes Magnitsky\\\")\n",
    "print(\\\"2. Alexandre de Moraes seria classificado no cluster de 'Choque Sist√™mico'\\\")\n",
    "print(\\\"3. Fatores de sentimento t√™m impacto significativo na magnitude da rea√ß√£o\\\")\n",
    "print(f\\\"4. O melhor modelo ({best_model_name}) apresentou R¬≤ de {model_results[best_model_name]['cv_mean']:.3f}\\\")\n",
    "\n",
    "print(\\\"\\\\n‚ö†Ô∏è CEN√ÅRIOS PREVISTOS PARA O BRASIL:\\\")\n",
    "for scenario, stats in final_results.items():\n",
    "    risk_level = \\\"ALTO\\\" if abs(stats['mean']) > 4 else \\\"M√âDIO\\\" if abs(stats['mean']) > 2 else \\\"BAIXO\\\"\n",
    "    print(f\\\"   {scenario.capitalize()}: {stats['mean']:.1f}% (Risco: {risk_level})\\\")\n",
    "\n",
    "print(\\\"\\\\nüéØ RECOMENDA√á√ïES PARA GESTORES DE RISCO:\\\")\n",
    "print(\\\"1. MONITORAMENTO: Acompanhar indicadores de sentimento da m√≠dia\\\")\n",
    "print(\\\"2. HEDGING: Considerar prote√ß√£o contra volatilidade em cen√°rios pessimistas\\\") \n",
    "print(\\\"3. LIQUIDEZ: Manter reservas para potencial fuga de capitais\\\")\n",
    "print(\\\"4. COMUNICA√á√ÉO: Preparar estrat√©gia de comunica√ß√£o para mercado\\\")\n",
    "print(\\\"5. DIVERSIFICA√á√ÉO: Considerar exposi√ß√£o a ativos internacionais\\\")\n",
    "\n",
    "print(\\\"\\\\nüìã LIMITA√á√ïES DO ESTUDO:\\\")\n",
    "print(\\\"‚Ä¢ Dataset limitado de casos hist√≥ricos (8 observa√ß√µes)\\\")\n",
    "print(\\\"‚Ä¢ Simula√ß√£o de features de sentimento (dados reais seriam prefer√≠veis)\\\")\n",
    "print(\\\"‚Ä¢ Modelo n√£o captura efeitos de segunda ordem ou cont√°gio\\\")\n",
    "print(\\\"‚Ä¢ Premissas sobre classifica√ß√£o de risco pol√≠tico podem variar\\\")\n",
    "\n",
    "print(\\\"\\\\n‚úÖ AN√ÅLISE COMPLETA FINALIZADA!\\\")\\nprint(\\\"üìä Dashboard executivo e relat√≥rio estat√≠stico gerados com sucesso.\\\")\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
